{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12682f68-a6ed-4038-96e1-0408f80bd3a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## NYC Yellow Taxi 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2458754a-8490-45c8-a377-3901414740d0",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"name\":291},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758993264221}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To view the root 'databricks-datasets' folder, run: `display(dbutils.fs.ls(\"/databricks-datasets/\"))`\n",
    "# display(dbutils.fs.ls(\"/databricks-datasets/nyctaxi/tripdata/yellow/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c11d64-1c5f-4dc9-9eb2-933e62ed1cb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### `NYCTaxiData` Class\n",
    "\n",
    "Loads and cleans NYC Yellow Taxi trip data for a given month and year. Provides trip counts and pickup aggregations by date or hour using Spark DataFrames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "431a7c46-609a-48ae-a3e9-de7099cccb15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, col, unix_timestamp, date_format, hour\n",
    "\n",
    "class NYCTaxiData:\n",
    "    def __init__(self, month_year_list):\n",
    "        self.month_year_list = month_year_list\n",
    "        self.clean_df = self.load_and_process_data()\n",
    "    \n",
    "    def read_data(self, month, year):\n",
    "        return spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .csv(f\"dbfs:/databricks-datasets/nyctaxi/tripdata/yellow/yellow_tripdata_{year}-{month}.csv.gz\")\n",
    "    \n",
    "    def clean_data(self, df, month, year):\n",
    "        next_month = (int(month) + 1) % 13\n",
    "        next_month = f\"0{next_month}\" if next_month < 10 else str(next_month)\n",
    "        year_of_next_month = year if month != \"12\" else f\"{int(year) + 1}\"\n",
    "        # print(f\"Cleaning data: The next_month is: {next_month}, year of the next month is: {year_of_next_month}\\n\")\n",
    "\n",
    "        # Drop rows if missing pickup or dropoff datetime\n",
    "        # Filter rows to include only trips in the year-month\n",
    "        # Exclude trips with invalid durations: negative or exceeding 24 hours (86400 seconds)\n",
    "        # Extract day and hour (military) from pickup and dropoff datetime\n",
    "        clean_df = df\\\n",
    "            .na.drop(subset=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"])\\\n",
    "            .filter(\n",
    "                (col(\"tpep_pickup_datetime\") >= f\"{year}-{month}-01\") & \n",
    "                (col(\"tpep_pickup_datetime\") < f\"{year_of_next_month}-{next_month}-01\") &\n",
    "                (col(\"tpep_dropoff_datetime\") >= f\"{year}-{month}-01\") & \n",
    "                (col(\"tpep_dropoff_datetime\") < f\"{year_of_next_month}-{next_month}-01\")\n",
    "            )\\\n",
    "            .filter(\n",
    "                (unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp(\"tpep_pickup_datetime\")).between(0, 86400)\n",
    "            )\\\n",
    "            .withColumn(\"pickup_date\", date_format(col(\"tpep_pickup_datetime\"), \"yyyy-MM-dd\"))\\\n",
    "            .withColumn(\"pickup_hour\", hour(col(\"tpep_pickup_datetime\")))\\\n",
    "            .withColumn(\"dropoff_date\", date_format(col(\"tpep_dropoff_datetime\"), \"yyyy-MM-dd\"))\\\n",
    "            .withColumn(\"dropoff_hour\", hour(col(\"tpep_dropoff_datetime\")))\n",
    "\n",
    "        return clean_df\n",
    "    \n",
    "    def load_and_process_data(self):\n",
    "        if not self.month_year_list:\n",
    "            raise ValueError(\"Empty list\")\n",
    "\n",
    "        dfs = []\n",
    "        for month, year in self.month_year_list:\n",
    "            data_df = self.read_data(month, year)\n",
    "            clean_df = self.clean_data(data_df, month, year)\n",
    "            dfs.append(clean_df)\n",
    "\n",
    "        union_df = dfs[0]\n",
    "        for df in dfs[1:]:\n",
    "            union_df = union_df.unionByName(df)\n",
    "        return union_df\n",
    "\n",
    "    def len(self):\n",
    "        return self.clean_df.count()\n",
    "    \n",
    "    # Return the top n hours with the most pickups\n",
    "    def pickups_per_time_interval(self, time_interval, top_n):\n",
    "        # time_interval either based on date or hour\n",
    "        pickups_per_time_interval = self.clean_df\\\n",
    "            .groupby(f\"pickup_{time_interval}\")\\\n",
    "            .count()\\\n",
    "            .withColumnRenamed(\"count\", \"num_pickups\") \\\n",
    "            .orderBy(\"num_pickups\", ascending=False)\\\n",
    "            .limit(top_n)\n",
    "        return pickups_per_time_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b1d21b1-c9ac-41fd-8c45-0692ee121f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "month_year_list = [(\"08\", \"2019\")]\n",
    "nyc_2019_08_df = NYCTaxiData(month_year_list)\n",
    "print(f\"Number of rows: {nyc_2019_08_df.len()}\")\n",
    "nyc_2019_08_df.pickups_per_time_interval(\"hour\", 5).show()\n",
    "nyc_2019_08_df.pickups_per_time_interval(\"date\", 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40f6071b-f372-4abe-925a-05cddb40a487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Forecast Taxi Demands\n",
    "Forecast taxi demand (number of rides) on a daily/hourly basis using 1 month of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f9143e7-0807-45fa-88c9-9f66839c3e1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ForecastTaxiDemands(NYCTaxiData):\n",
    "    def __init__(self, month_year_list):\n",
    "        super().__init__(month_year_list)\n",
    "        self.aggregate_df = self.aggregate_data()\n",
    "    \n",
    "    def aggregate_data(self):\n",
    "        \"\"\"Aggregate pickups by date and time\"\"\"\n",
    "        aggregate_df = self.clean_df \\\n",
    "                        .groupBy(\"pickup_date\", \"pickup_hour\") \\\n",
    "                        .count() \\\n",
    "                        .withColumnRenamed(\"count\", \"num_pickups\") \\\n",
    "                        .orderBy(\"pickup_date\", \"pickup_hour\")\n",
    "        return aggregate_df\n",
    "    \n",
    "    def convert_to_pandas(self):\n",
    "        return self.aggregate_df.toPandas()\n",
    "    \n",
    "    def len(self):\n",
    "        return self.aggregate_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8ec80ea-681c-4dc6-bc79-5a05dad3cd38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Baseline: ARIMA\n",
    "- **I:** Integration (Differencing)\n",
    "    - Makes the data stationary by modeling differences between consecutive observations. If \\( d = 1 \\), the model predicts the difference between current and previous data points.\n",
    "- **AR:** AutoRegressive\n",
    "    - Models the current value as a weighted sum of past values (lags).\n",
    "- **MA:** Moving Average\n",
    "    - Models the current value as a weighted sum of past forecast errors (shocks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e29bf99-d491-4a54-9e08-97417130f6e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44e832f4-01cd-4d7d-b7f7-e68fa140afa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d7b2f17-3cb1-4816-b3d5-e076f7af830d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pmdarima as pm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ARIMAForecaster:\n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - data (pd.DataFrame): Must include a 'num_pickups' column.\n",
    "        \"\"\"\n",
    "        self.original_data = data.reset_index(drop=True)\n",
    "        self.train, self.test = self._split_data()\n",
    "        self.model = None\n",
    "\n",
    "    def _split_data(self, split_ratio: float = 0.8):\n",
    "        idx = int(split_ratio * len(self.original_data))\n",
    "        # Only log-transform the training data\n",
    "        train_log = np.log(self.original_data[\"num_pickups\"][:-48] + 1)\n",
    "        test_actual = self.original_data[\"num_pickups\"][-48:]\n",
    "        return train_log, test_actual\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Fits auto ARIMA model to log-transformed training data.\"\"\"\n",
    "        self.model = pm.auto_arima(\n",
    "            self.train,\n",
    "            start_p=0, max_p=3,\n",
    "            start_q=0, max_q=3,\n",
    "            d=None,\n",
    "            start_P=0, max_P=1,\n",
    "            start_Q=0, max_Q=1,\n",
    "            D=None,\n",
    "            seasonal=True,\n",
    "            m=24,\n",
    "            stepwise=True,\n",
    "            suppress_warnings=True,\n",
    "            error_action=\"ignore\"\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def forecast(self, steps: int = None) -> np.ndarray:\n",
    "        \"\"\"Forecasts and returns predictions in the original scale.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not fit yet.\")\n",
    "        if steps is None:\n",
    "            steps = len(self.test)\n",
    "        log_preds = self.model.predict(n_periods=steps)\n",
    "        return np.exp(log_preds) - 1  # inverse log-transform\n",
    "\n",
    "    def evaluate(self) -> float:\n",
    "        \"\"\"Returns RMSE between predicted and actual values (original scale).\"\"\"\n",
    "        y_pred = self.forecast()\n",
    "        y_true = self.test.values\n",
    "        rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        return rmse, mape\n",
    "    \n",
    "    def plot_forecast_error(self):\n",
    "        y_pred = self.forecast()\n",
    "        y_true = self.test.values\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(np.abs(y_true - y_pred), label='Actual', marker='o')\n",
    "        plt.title('ARIMA Forecast Error')\n",
    "        plt.xlabel('Time (hour index)')\n",
    "        plt.ylabel('Error (absolute)')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fe6dd0-b14f-4d1b-8187-7757d5054665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "month_year_list = []\n",
    "for i in range(4, 11):\n",
    "  month = f\"0{i}\" if i < 10 else f\"{i}\"\n",
    "  month_year_list.append((month, \"2019\"))\n",
    "\n",
    "data = ForecastTaxiDemands(month_year_list).convert_to_pandas()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f61d3c4-870d-4744-8b51-f77a68917d0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Forecast the next 48 hours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26b42c23-e663-4311-a1e2-1d1aa3231f84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecaster = ARIMAForecaster(data).fit()\n",
    "\n",
    "rmse, mape = forecaster.evaluate()\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAPE: {mape:.4f}\")\n",
    "# RMSE: 6146.1763, 3111\n",
    "# MAPE: 80.4993\n",
    "\n",
    "# RMSE: 4433.4665\n",
    "# MAPE: 105.2355"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28d77703-728f-4473-8576-245e9cb72c9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "119bcb1f-07e6-42c9-806b-eb74d6e924a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7904567-62ed-40cb-bb58-6fa436e18bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50041421-c641-4aac-813a-6ad146be806d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "\n",
    "def split_train_test(data):\n",
    "    data['pickup_date'] = pd.to_datetime(data['pickup_date'])  # convert date column to datetime\n",
    "    data['ds'] = data['pickup_date'] + pd.to_timedelta(data['pickup_hour'], unit='h')\n",
    "\n",
    "    df = data[['ds', 'num_pickups']].copy()\n",
    "    df.rename(columns={'num_pickups': 'y'}, inplace=True)\n",
    "\n",
    "    # log-transform the training set\n",
    "    df['y_orig'] = df['y']  # keep original\n",
    "    df['y'] = np.log(df['y'] + 1)\n",
    "\n",
    "    train = df.iloc[:-48]\n",
    "    test = df.iloc[-48:]\n",
    "    return train, test\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    # Reverse log transform to original scale\n",
    "    y_pred_exp = np.exp(y_pred) - 1\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred_exp) ** 2))\n",
    "    mape = np.mean(np.abs((y_true - y_pred_exp) / y_true)) * 100\n",
    "    return rmse, mape\n",
    "\n",
    "# Assuming 'data' is your full DataFrame with pickup_date and num_pickups columns\n",
    "train, test = split_train_test(data)\n",
    "\n",
    "best_rmse = float('inf')\n",
    "best_mape = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "for sps in [4, 6, 8, 10]:\n",
    "    for cps in [0.01, 0.05, 0.1]:\n",
    "        for fourier_order in [3, 5, 7]:\n",
    "            model = Prophet(\n",
    "                daily_seasonality=True, \n",
    "                weekly_seasonality=True, \n",
    "                changepoint_prior_scale=cps,\n",
    "                seasonality_prior_scale=sps\n",
    "            )\n",
    "            model.add_seasonality(name='hourly', period=24, fourier_order=7)  # capture hourly pattern\n",
    "            model.fit(train)\n",
    "\n",
    "            future = model.make_future_dataframe(periods=48, freq='H')\n",
    "            forecast = model.predict(future)\n",
    "\n",
    "            # Align since prophet forecasts both TRAINING period and future\n",
    "            pred_df = forecast[['ds', 'yhat']].merge(test[['ds', 'y_orig']], on='ds', how='inner')\n",
    "            rmse, mape = evaluate(pred_df['y_orig'].values, pred_df['yhat'].values)\n",
    "            print(f\"RMSE: {rmse:.4f}\")\n",
    "            print(f\"MAPE: {mape:.4f}\")\n",
    "            \n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_mape = mape\n",
    "                best_params = {'changepoint_prior_scale': cps, \n",
    "                               'fourier_order': fourier_order, \n",
    "                               'seasonality_prior_scale': sps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "353efb42-7982-4dbe-b168-3c5112c2b6c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best params:\", best_params)\n",
    "print(\"Best RMSE:\", best_rmse)\n",
    "print(\"Best MAPE:\", best_mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf787780-b3e2-4dff-8684-086a8f041568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NYC Yellow Taxi",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
